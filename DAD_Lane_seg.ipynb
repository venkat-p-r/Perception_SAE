{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO0qWuQyIsE/2n/R/62FmyB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6cGxewP_piSL","executionInfo":{"status":"ok","timestamp":1675585052876,"user_tz":-330,"elapsed":19846,"user":{"displayName":"SENTHIL NATHAN","userId":"08448263993860017588"}},"outputId":"8b8af613-2222-4332-ae88-ad1a0ae72b02"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import cv2 # Opencv Module\n","import torch # Pytorch"],"metadata":{"id":"cHeODmPzprzM","executionInfo":{"status":"ok","timestamp":1675585060570,"user_tz":-330,"elapsed":4373,"user":{"displayName":"SENTHIL NATHAN","userId":"08448263993860017588"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["os.chdir('gdrive/My Drive/Classroom/SAE Workshop')"],"metadata":{"id":"uzlpoRxRpuyx","executionInfo":{"status":"ok","timestamp":1675585071756,"user_tz":-330,"elapsed":524,"user":{"displayName":"SENTHIL NATHAN","userId":"08448263993860017588"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/hustvl/YOLOP.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iSTXEagRp1kC","executionInfo":{"status":"ok","timestamp":1675587583170,"user_tz":-330,"elapsed":19836,"user":{"displayName":"SENTHIL NATHAN","userId":"08448263993860017588"}},"outputId":"4a1f08d6-deb6-4005-fb68-f4db265a350e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'YOLOP'...\n","remote: Enumerating objects: 428, done.\u001b[K\n","remote: Counting objects: 100% (428/428), done.\u001b[K\n","remote: Compressing objects: 100% (228/228), done.\u001b[K\n","remote: Total 428 (delta 203), reused 376 (delta 191), pack-reused 0\u001b[K\n","Receiving objects: 100% (428/428), 140.18 MiB | 14.99 MiB/s, done.\n","Resolving deltas: 100% (203/203), done.\n","Updating files: 100% (95/95), done.\n"]}]},{"cell_type":"code","source":["cd YOLOP"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O9FWGMSezpfB","executionInfo":{"status":"ok","timestamp":1675587661510,"user_tz":-330,"elapsed":3,"user":{"displayName":"SENTHIL NATHAN","userId":"08448263993860017588"}},"outputId":"17907806-ab30-43e2-8642-c1879b051f50"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP\n"]}]},{"cell_type":"code","source":["!pip install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NtwNYfRozilO","executionInfo":{"status":"ok","timestamp":1675587674739,"user_tz":-330,"elapsed":4369,"user":{"displayName":"SENTHIL NATHAN","userId":"08448263993860017588"}},"outputId":"0e8276dc-40c0-4bb2-f622-9448af7d9364"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (1.7.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (4.64.1)\n","Collecting yacs\n","  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Requirement already satisfied: Cython in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (0.29.33)\n","Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 5)) (3.2.2)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (1.21.6)\n","Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 7)) (4.6.0.66)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 8)) (7.1.2)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 9)) (6.0)\n","Collecting tensorboardX\n","  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.4/125.4 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 12)) (0.11.2)\n","Requirement already satisfied: prefetch_generator in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 13)) (1.0.3)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 14)) (2.9.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 15)) (1.0.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (1.4.4)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (0.11.0)\n","Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX->-r requirements.txt (line 11)) (3.19.6)\n","Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.8/dist-packages (from seaborn->-r requirements.txt (line 12)) (1.3.5)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->-r requirements.txt (line 15)) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->-r requirements.txt (line 15)) (3.1.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.23->seaborn->-r requirements.txt (line 12)) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib>=3.2.2->-r requirements.txt (line 5)) (1.15.0)\n","Installing collected packages: yacs, tensorboardX\n","Successfully installed tensorboardX-2.5.1 yacs-0.1.8\n"]}]},{"cell_type":"code","source":["model = torch.hub.load('hustvl/yolop', 'yolop', pretrained=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YsMbOcM80E0N","executionInfo":{"status":"ok","timestamp":1675587819897,"user_tz":-330,"elapsed":19610,"user":{"displayName":"SENTHIL NATHAN","userId":"08448263993860017588"}},"outputId":"12d5b7f4-932b-48b9-ae29-8df8166340ee"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torch/hub.py:267: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n","  warnings.warn(\n","Downloading: \"https://github.com/hustvl/yolop/zipball/main\" to /root/.cache/torch/hub/main.zip\n"]}]},{"cell_type":"code","source":["!python tools/demo_mod.py --source inference/images"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vEx5115X0Pmb","executionInfo":{"status":"ok","timestamp":1675588504190,"user_tz":-330,"elapsed":14110,"user":{"displayName":"SENTHIL NATHAN","userId":"08448263993860017588"}},"outputId":"3988502f-50e9-41a6-8fc4-108c97f25c26"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["['/content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/tools', '/env/python', '/usr/lib/python38.zip', '/usr/lib/python3.8', '/usr/lib/python3.8/lib-dynload', '/usr/local/lib/python3.8/dist-packages', '/usr/lib/python3/dist-packages', '/content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP']\n","=> creating runs/BddDataset/_2023-02-05-09-15\n","Using torch 1.13.1+cu116 CPU\n","\n","  0% 0/6 [00:00<?, ?it/s]image 1/6 /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/images/0ace96c3-48481887.jpg: \n","<class 'tuple'> ((720, 1280), ((0.5333333333333333, 0.5), (0.0, 12.0)))\n","/usr/local/lib/python3.8/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","0.5\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (720, 1280, 3)\n","384 640 0 12\n"," 17% 1/6 [00:01<00:07,  1.43s/it]image 2/6 /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/images/3c0e7240-96e390d2.jpg: \n","<class 'tuple'> ((720, 1280), ((0.5333333333333333, 0.5), (0.0, 12.0)))\n","0.5\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (720, 1280, 3)\n","384 640 0 12\n"," 33% 2/6 [00:02<00:04,  1.19s/it]image 3/6 /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/images/7dd9ef45-f197db95.jpg: \n","<class 'tuple'> ((720, 1280), ((0.5333333333333333, 0.5), (0.0, 12.0)))\n","0.5\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (720, 1280, 3)\n","384 640 0 12\n"," 50% 3/6 [00:03<00:02,  1.04it/s]image 4/6 /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/images/8e1c1ab0-a8b92173.jpg: \n","<class 'tuple'> ((720, 1280), ((0.5333333333333333, 0.5), (0.0, 12.0)))\n","0.5\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (720, 1280, 3)\n","384 640 0 12\n"," 67% 4/6 [00:03<00:01,  1.16it/s]image 5/6 /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/images/9aa94005-ff1d4c9a.jpg: \n","<class 'tuple'> ((720, 1280), ((0.5333333333333333, 0.5), (0.0, 12.0)))\n","0.5\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (720, 1280, 3)\n","384 640 0 12\n"," 83% 5/6 [00:04<00:00,  1.27it/s]image 6/6 /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/images/adb4871d-4d063244.jpg: \n","<class 'tuple'> ((720, 1280), ((0.5333333333333333, 0.5), (0.0, 12.0)))\n","0.5\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (720, 1280, 3)\n","384 640 0 12\n","100% 6/6 [00:05<00:00,  1.15it/s]\n","Results saved to inference/output\n","Done. (5.246s)\n","inf : (0.6372s/frame)   nms : (0.0040s/frame)\n"]}]},{"cell_type":"code","source":["ll_seg = np.load('ll.npy')\n","plt.imshow(ll_seg, cmap='gray')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":258},"id":"N0NQbeG94HR5","executionInfo":{"status":"ok","timestamp":1675588886980,"user_tz":-330,"elapsed":1182,"user":{"displayName":"SENTHIL NATHAN","userId":"08448263993860017588"}},"outputId":"1426b3b4-7a67-439a-f9af-2212d6ea41be"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7fa209c16f10>"]},"metadata":{},"execution_count":12},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["detections = np.load('detections.npy')\n","print(detections)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QY-7Ml2F4gl6","executionInfo":{"status":"ok","timestamp":1675588955306,"user_tz":-330,"elapsed":4,"user":{"displayName":"SENTHIL NATHAN","userId":"08448263993860017588"}},"outputId":"bda92714-e3ff-409a-fd17-ca7311ab7645"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["[[4.3761258e+02 1.8847116e+02 5.6030365e+02 2.5539160e+02 9.2106980e-01\n","  0.0000000e+00]\n"," [3.7276077e+00 2.0359137e+02 1.3424435e+02 2.6948059e+02 9.1323680e-01\n","  0.0000000e+00]\n"," [1.0603267e+02 1.8536502e+02 1.8764525e+02 2.4093951e+02 9.0140730e-01\n","  0.0000000e+00]\n"," [3.9800372e+02 1.8045639e+02 4.6565021e+02 2.3265073e+02 8.2767141e-01\n","  0.0000000e+00]\n"," [1.8360526e+02 1.9664536e+02 2.1341299e+02 2.2804868e+02 8.1174678e-01\n","  0.0000000e+00]\n"," [2.6824460e+02 1.8018767e+02 3.1675211e+02 2.1833885e+02 8.0198789e-01\n","  0.0000000e+00]\n"," [3.9089053e+02 1.8508588e+02 4.1256992e+02 2.2036981e+02 7.7999765e-01\n","  0.0000000e+00]\n"," [3.6625363e+02 1.6697707e+02 4.1134067e+02 2.1011046e+02 7.1323395e-01\n","  0.0000000e+00]\n"," [2.0366298e+02 1.9562555e+02 2.3786476e+02 2.2213748e+02 7.1043646e-01\n","  0.0000000e+00]\n"," [2.5484406e+02 1.9517111e+02 2.7512347e+02 2.1556238e+02 6.3938618e-01\n","  0.0000000e+00]\n"," [2.3450664e+02 1.9758231e+02 2.5959360e+02 2.1825616e+02 6.2179959e-01\n","  0.0000000e+00]\n"," [3.2916125e+02 1.8923154e+02 3.6869580e+02 2.0959457e+02 6.0574812e-01\n","  0.0000000e+00]\n"," [3.5303879e+02 1.9032185e+02 3.6985782e+02 2.0741307e+02 3.0255598e-01\n","  0.0000000e+00]\n"," [4.1474521e+02 1.7224548e+02 4.3243820e+02 1.8136023e+02 2.9176727e-01\n","  0.0000000e+00]]\n"]}]},{"cell_type":"code","source":["dad_seg = np.load('dad.npy')\n","plt.imshow(dad_seg, cmap='gray')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":258},"id":"2N7L0S-B4W2Z","executionInfo":{"status":"ok","timestamp":1675588904826,"user_tz":-330,"elapsed":831,"user":{"displayName":"SENTHIL NATHAN","userId":"08448263993860017588"}},"outputId":"071c6862-1dc8-46e4-d543-097c9c9e74e4"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7fa209b9cd00>"]},"metadata":{},"execution_count":13},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["!python tools/demo_mod.py --source inference/videos"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OJbX3ZAM4y7_","outputId":"70f5b7a1-7965-4bd8-a616-43e512bf9526"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['/content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/tools', '/env/python', '/usr/lib/python38.zip', '/usr/lib/python3.8', '/usr/lib/python3.8/lib-dynload', '/usr/local/lib/python3.8/dist-packages', '/usr/lib/python3/dist-packages', '/content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP']\n","=> creating runs/BddDataset/_2023-02-05-09-23\n","Using torch 1.13.1+cu116 CPU\n","\n","  0% 0/1 [00:00<?, ?it/s]\n"," video 1/1 (1/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","/usr/local/lib/python3.8/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","100% 1/1 [00:01<00:00,  1.54s/it]\n"," video 1/1 (2/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","2it [00:02,  1.41s/it]           \n"," video 1/1 (3/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","3it [00:04,  1.33s/it]\n"," video 1/1 (4/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","4it [00:05,  1.17s/it]\n"," video 1/1 (5/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","5it [00:05,  1.05s/it]\n"," video 1/1 (6/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","6it [00:06,  1.03it/s]\n"," video 1/1 (7/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","7it [00:07,  1.09it/s]\n"," video 1/1 (8/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","8it [00:08,  1.13it/s]\n"," video 1/1 (9/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","9it [00:09,  1.17it/s]\n"," video 1/1 (10/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","10it [00:09,  1.19it/s]\n"," video 1/1 (11/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","11it [00:10,  1.11it/s]\n"," video 1/1 (12/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","12it [00:11,  1.15it/s]\n"," video 1/1 (13/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","13it [00:12,  1.19it/s]\n"," video 1/1 (14/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","14it [00:13,  1.21it/s]\n"," video 1/1 (15/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","15it [00:14,  1.23it/s]\n"," video 1/1 (16/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","16it [00:15,  1.11it/s]\n"," video 1/1 (17/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","17it [00:16,  1.00it/s]\n"," video 1/1 (18/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","18it [00:17,  1.06s/it]\n"," video 1/1 (19/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","19it [00:18,  1.10s/it]\n"," video 1/1 (20/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","20it [00:19,  1.02s/it]\n"," video 1/1 (21/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","21it [00:20,  1.05it/s]\n"," video 1/1 (22/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","22it [00:21,  1.10it/s]\n"," video 1/1 (23/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","23it [00:22,  1.10it/s]\n"," video 1/1 (24/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","24it [00:22,  1.13it/s]\n"," video 1/1 (25/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","25it [00:23,  1.16it/s]\n"," video 1/1 (26/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","26it [00:24,  1.19it/s]\n"," video 1/1 (27/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","27it [00:25,  1.19it/s]\n"," video 1/1 (28/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","28it [00:26,  1.20it/s]\n"," video 1/1 (29/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","29it [00:27,  1.21it/s]\n"," video 1/1 (30/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","30it [00:27,  1.22it/s]\n"," video 1/1 (31/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","31it [00:28,  1.22it/s]\n"," video 1/1 (32/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","32it [00:29,  1.12it/s]\n"," video 1/1 (33/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","33it [00:30,  1.01it/s]\n"," video 1/1 (34/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","34it [00:32,  1.07s/it]\n"," video 1/1 (35/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","35it [00:33,  1.11s/it]\n"," video 1/1 (36/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","36it [00:34,  1.05s/it]\n"," video 1/1 (37/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","37it [00:35,  1.02it/s]\n"," video 1/1 (38/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","38it [00:35,  1.09it/s]\n"," video 1/1 (39/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","39it [00:36,  1.13it/s]\n"," video 1/1 (40/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","40it [00:37,  1.17it/s]\n"," video 1/1 (41/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","41it [00:38,  1.20it/s]\n"," video 1/1 (42/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","42it [00:39,  1.16it/s]\n"," video 1/1 (43/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","43it [00:40,  1.19it/s]\n"," video 1/1 (44/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","44it [00:41,  1.02it/s]\n"," video 1/1 (45/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","45it [00:42,  1.07s/it]\n"," video 1/1 (46/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","46it [00:43,  1.01it/s]\n"," video 1/1 (47/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","47it [00:44,  1.03s/it]\n"," video 1/1 (48/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","48it [00:45,  1.12s/it]\n"," video 1/1 (49/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","49it [00:47,  1.14s/it]\n"," video 1/1 (50/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","50it [00:48,  1.14s/it]\n"," video 1/1 (51/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","51it [00:49,  1.04s/it]\n"," video 1/1 (52/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","52it [00:49,  1.02it/s]\n"," video 1/1 (53/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","53it [00:50,  1.08it/s]\n"," video 1/1 (54/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","54it [00:51,  1.11it/s]\n"," video 1/1 (55/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","55it [00:52,  1.14it/s]\n"," video 1/1 (56/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","56it [00:53,  1.16it/s]\n"," video 1/1 (57/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","57it [00:53,  1.18it/s]\n"," video 1/1 (58/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","58it [00:54,  1.19it/s]\n"," video 1/1 (59/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","59it [00:56,  1.02it/s]\n"," video 1/1 (60/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","60it [00:57,  1.15s/it]\n"," video 1/1 (61/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","61it [00:58,  1.09s/it]\n"," video 1/1 (62/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","62it [00:59,  1.12s/it]\n"," video 1/1 (63/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","63it [01:00,  1.15s/it]\n"," video 1/1 (64/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","64it [01:02,  1.17s/it]\n"," video 1/1 (65/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","65it [01:03,  1.14s/it]\n"," video 1/1 (66/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","66it [01:04,  1.05s/it]\n"," video 1/1 (67/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","67it [01:04,  1.03it/s]\n"," video 1/1 (68/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","68it [01:05,  1.08it/s]\n"," video 1/1 (69/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","69it [01:06,  1.13it/s]\n"," video 1/1 (70/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","70it [01:07,  1.02it/s]\n"," video 1/1 (71/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","71it [01:08,  1.04s/it]\n"," video 1/1 (72/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","72it [01:09,  1.02it/s]\n"," video 1/1 (73/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","73it [01:10,  1.07it/s]\n"," video 1/1 (74/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","74it [01:11,  1.12it/s]\n"," video 1/1 (75/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","75it [01:12,  1.15it/s]\n"," video 1/1 (76/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","76it [01:13,  1.15it/s]\n"," video 1/1 (77/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","77it [01:14,  1.03it/s]\n"," video 1/1 (78/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","78it [01:15,  1.05s/it]\n"," video 1/1 (79/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","79it [01:16,  1.10s/it]\n"," video 1/1 (80/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","80it [01:17,  1.12s/it]\n"," video 1/1 (81/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","81it [01:18,  1.03s/it]\n"," video 1/1 (82/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","82it [01:19,  1.04it/s]\n"," video 1/1 (83/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","83it [01:20,  1.07it/s]\n"," video 1/1 (84/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","84it [01:21,  1.12it/s]\n"," video 1/1 (85/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","85it [01:22,  1.13it/s]\n"," video 1/1 (86/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","86it [01:22,  1.15it/s]\n"," video 1/1 (87/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","87it [01:23,  1.17it/s]\n"," video 1/1 (88/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","88it [01:24,  1.19it/s]\n"," video 1/1 (89/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","89it [01:25,  1.20it/s]\n"," video 1/1 (90/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","90it [01:26,  1.21it/s]\n"," video 1/1 (91/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","91it [01:26,  1.21it/s]\n"," video 1/1 (92/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","92it [01:27,  1.21it/s]\n"," video 1/1 (93/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","93it [01:28,  1.08it/s]\n"," video 1/1 (94/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","94it [01:30,  1.03s/it]\n"," video 1/1 (95/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","95it [01:31,  1.09s/it]\n"," video 1/1 (96/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","96it [01:32,  1.11s/it]\n"," video 1/1 (97/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","97it [01:33,  1.07s/it]\n"," video 1/1 (98/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","98it [01:34,  1.01it/s]\n"," video 1/1 (99/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","99it [01:35,  1.07it/s]\n"," video 1/1 (100/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","100it [01:35,  1.12it/s]\n"," video 1/1 (101/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","101it [01:36,  1.15it/s]\n"," video 1/1 (102/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","102it [01:37,  1.19it/s]\n"," video 1/1 (103/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","103it [01:38,  1.21it/s]\n"," video 1/1 (104/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","104it [01:39,  1.15it/s]\n"," video 1/1 (105/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","105it [01:40,  1.18it/s]\n"," video 1/1 (106/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","106it [01:40,  1.20it/s]\n"," video 1/1 (107/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","107it [01:41,  1.19it/s]\n"," video 1/1 (108/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","108it [01:42,  1.19it/s]\n"," video 1/1 (109/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","109it [01:43,  1.05it/s]\n"," video 1/1 (110/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","110it [01:45,  1.05s/it]\n"," video 1/1 (111/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","111it [01:46,  1.09s/it]\n"," video 1/1 (112/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","112it [01:47,  1.09s/it]\n"," video 1/1 (113/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","113it [01:48,  1.01s/it]\n"," video 1/1 (114/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","114it [01:49,  1.05it/s]\n"," video 1/1 (115/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","115it [01:49,  1.10it/s]\n"," video 1/1 (116/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","116it [01:50,  1.12it/s]\n"," video 1/1 (117/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","117it [01:51,  1.15it/s]\n"," video 1/1 (118/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","118it [01:52,  1.16it/s]\n"," video 1/1 (119/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","119it [01:53,  1.18it/s]\n"," video 1/1 (120/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","120it [01:53,  1.20it/s]\n"," video 1/1 (121/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","121it [01:54,  1.22it/s]\n"," video 1/1 (122/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","122it [01:55,  1.21it/s]\n"," video 1/1 (123/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","123it [01:56,  1.22it/s]\n"," video 1/1 (124/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","124it [01:57,  1.20it/s]\n"," video 1/1 (125/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","125it [01:58,  1.07it/s]\n"," video 1/1 (126/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","126it [01:59,  1.02s/it]\n"," video 1/1 (127/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","127it [02:00,  1.07s/it]\n"," video 1/1 (128/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","128it [02:02,  1.11s/it]\n"," video 1/1 (129/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","129it [02:02,  1.01s/it]\n"," video 1/1 (130/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","130it [02:03,  1.05it/s]\n"," video 1/1 (131/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","131it [02:04,  1.10it/s]\n"," video 1/1 (132/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","132it [02:05,  1.14it/s]\n"," video 1/1 (133/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","133it [02:06,  1.17it/s]\n"," video 1/1 (134/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","134it [02:07,  1.08it/s]\n"," video 1/1 (135/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","135it [02:07,  1.13it/s]\n"," video 1/1 (136/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","136it [02:08,  1.16it/s]\n"," video 1/1 (137/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","137it [02:09,  1.19it/s]\n"," video 1/1 (138/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","138it [02:10,  1.20it/s]\n"," video 1/1 (139/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","139it [02:11,  1.22it/s]\n"," video 1/1 (140/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","140it [02:12,  1.06it/s]\n"," video 1/1 (141/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","141it [02:13,  1.01s/it]\n"," video 1/1 (142/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","142it [02:14,  1.08s/it]\n"," video 1/1 (143/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","143it [02:15,  1.12s/it]\n"," video 1/1 (144/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","144it [02:16,  1.06s/it]\n"," video 1/1 (145/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","145it [02:17,  1.02it/s]\n"," video 1/1 (146/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","146it [02:18,  1.07it/s]\n"," video 1/1 (147/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","147it [02:19,  1.05it/s]\n"," video 1/1 (148/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","148it [02:20,  1.10it/s]\n"," video 1/1 (149/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","149it [02:21,  1.13it/s]\n"," video 1/1 (150/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","150it [02:22,  1.15it/s]\n"," video 1/1 (151/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","151it [02:22,  1.16it/s]\n"," video 1/1 (152/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","152it [02:23,  1.17it/s]\n"," video 1/1 (153/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","153it [02:24,  1.14it/s]\n"," video 1/1 (154/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","154it [02:25,  1.17it/s]\n"," video 1/1 (155/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","155it [02:26,  1.18it/s]\n"," video 1/1 (156/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","156it [02:27,  1.04it/s]\n"," video 1/1 (157/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","157it [02:28,  1.03s/it]\n"," video 1/1 (158/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","158it [02:29,  1.09s/it]\n"," video 1/1 (159/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","159it [02:31,  1.16s/it]\n"," video 1/1 (160/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","160it [02:32,  1.06s/it]\n"," video 1/1 (161/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","161it [02:32,  1.01it/s]\n"," video 1/1 (162/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","162it [02:33,  1.05it/s]\n"," video 1/1 (163/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","163it [02:34,  1.10it/s]\n"," video 1/1 (164/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","164it [02:35,  1.13it/s]\n"," video 1/1 (165/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","165it [02:36,  1.04it/s]\n"," video 1/1 (166/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","166it [02:37,  1.09it/s]\n"," video 1/1 (167/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","167it [02:38,  1.12it/s]\n"," video 1/1 (168/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","168it [02:38,  1.15it/s]\n"," video 1/1 (169/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","169it [02:39,  1.16it/s]\n"," video 1/1 (170/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","170it [02:40,  1.18it/s]\n"," video 1/1 (171/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","171it [02:41,  1.01it/s]\n"," video 1/1 (172/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","172it [02:43,  1.07s/it]\n"," video 1/1 (173/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","173it [02:44,  1.12s/it]\n"," video 1/1 (174/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","174it [02:45,  1.14s/it]\n"," video 1/1 (175/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","175it [02:46,  1.05s/it]\n"," video 1/1 (176/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","176it [02:47,  1.03s/it]\n"," video 1/1 (177/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","177it [02:48,  1.03it/s]\n"," video 1/1 (178/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","178it [02:49,  1.05it/s]\n"," video 1/1 (179/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","179it [02:50,  1.09it/s]\n"," video 1/1 (180/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","180it [02:50,  1.12it/s]\n"," video 1/1 (181/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","181it [02:51,  1.15it/s]\n"," video 1/1 (182/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","182it [02:52,  1.16it/s]\n"," video 1/1 (183/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","183it [02:53,  1.17it/s]\n"," video 1/1 (184/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","184it [02:54,  1.17it/s]\n"," video 1/1 (185/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","185it [02:55,  1.17it/s]\n"," video 1/1 (186/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","186it [02:56,  1.10it/s]\n"," video 1/1 (187/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","187it [02:57,  1.01s/it]\n"," video 1/1 (188/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","188it [02:58,  1.07s/it]\n"," video 1/1 (189/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","189it [02:59,  1.11s/it]\n"," video 1/1 (190/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","190it [03:00,  1.12s/it]\n"," video 1/1 (191/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","191it [03:01,  1.03s/it]\n"," video 1/1 (192/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","192it [03:02,  1.04it/s]\n"," video 1/1 (193/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","193it [03:03,  1.08it/s]\n"," video 1/1 (194/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","194it [03:04,  1.12it/s]\n"," video 1/1 (195/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","195it [03:05,  1.13it/s]\n"," video 1/1 (196/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","196it [03:05,  1.16it/s]\n"," video 1/1 (197/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","197it [03:06,  1.12it/s]\n"," video 1/1 (198/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","198it [03:07,  1.14it/s]\n"," video 1/1 (199/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","199it [03:08,  1.16it/s]\n"," video 1/1 (200/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","200it [03:09,  1.17it/s]\n"," video 1/1 (201/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","201it [03:10,  1.17it/s]\n"," video 1/1 (202/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","202it [03:11,  1.05it/s]\n"," video 1/1 (203/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","203it [03:12,  1.07s/it]\n"," video 1/1 (204/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","204it [03:13,  1.12s/it]\n"," video 1/1 (205/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","205it [03:15,  1.13s/it]\n"," video 1/1 (206/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","206it [03:15,  1.04s/it]\n"," video 1/1 (207/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","207it [03:16,  1.03it/s]\n"," video 1/1 (208/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","208it [03:17,  1.07it/s]\n"," video 1/1 (209/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","209it [03:18,  1.06it/s]\n"," video 1/1 (210/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","210it [03:19,  1.10it/s]\n"," video 1/1 (211/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","211it [03:20,  1.13it/s]\n"," video 1/1 (212/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","212it [03:21,  1.14it/s]\n"," video 1/1 (213/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","213it [03:21,  1.17it/s]\n"," video 1/1 (214/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","214it [03:22,  1.18it/s]\n"," video 1/1 (215/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","215it [03:23,  1.12it/s]\n"," video 1/1 (216/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","216it [03:24,  1.15it/s]\n"," video 1/1 (217/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","217it [03:25,  1.08it/s]\n"," video 1/1 (218/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","218it [03:26,  1.02s/it]\n"," video 1/1 (219/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","219it [03:28,  1.07s/it]\n"," video 1/1 (220/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","220it [03:29,  1.11s/it]\n"," video 1/1 (221/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","221it [03:30,  1.10s/it]\n"," video 1/1 (222/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","222it [03:31,  1.01s/it]\n"," video 1/1 (223/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","223it [03:31,  1.05it/s]\n"," video 1/1 (224/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","224it [03:32,  1.10it/s]\n"," video 1/1 (225/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","225it [03:33,  1.14it/s]\n"," video 1/1 (226/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","226it [03:34,  1.15it/s]\n"," video 1/1 (227/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","227it [03:35,  1.06it/s]\n"," video 1/1 (228/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","228it [03:36,  1.11it/s]\n"," video 1/1 (229/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","229it [03:37,  1.12it/s]\n"," video 1/1 (230/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","230it [03:37,  1.15it/s]\n"," video 1/1 (231/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","231it [03:38,  1.17it/s]\n"," video 1/1 (232/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","232it [03:39,  1.18it/s]\n"," video 1/1 (233/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","233it [03:40,  1.05it/s]\n"," video 1/1 (234/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","234it [03:42,  1.02s/it]\n"," video 1/1 (235/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","235it [03:43,  1.08s/it]\n"," video 1/1 (236/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","236it [03:44,  1.11s/it]\n"," video 1/1 (237/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","237it [03:45,  1.02s/it]\n"," video 1/1 (238/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","238it [03:46,  1.04it/s]\n"," video 1/1 (239/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","239it [03:46,  1.09it/s]\n"," video 1/1 (240/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","240it [03:47,  1.12it/s]\n"," video 1/1 (241/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","241it [03:48,  1.15it/s]\n"," video 1/1 (242/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","242it [03:49,  1.18it/s]\n"," video 1/1 (243/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","243it [03:50,  1.19it/s]\n"," video 1/1 (244/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","244it [03:50,  1.20it/s]\n"," video 1/1 (245/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","245it [03:51,  1.22it/s]\n"," video 1/1 (246/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","246it [03:52,  1.21it/s]\n"," video 1/1 (247/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","247it [03:53,  1.21it/s]\n"," video 1/1 (248/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","248it [03:54,  1.22it/s]\n"," video 1/1 (249/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","249it [03:55,  1.10it/s]\n"," video 1/1 (250/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","250it [03:56,  1.02it/s]\n"," video 1/1 (251/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","251it [03:57,  1.04s/it]\n"," video 1/1 (252/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n","0.3333333333333333\n","<class 'torch.Tensor'> torch.Size([1, 3, 384, 640])\n","<class 'numpy.ndarray'> (1080, 1920, 3)\n","384 640 0 12\n","252it [03:58,  1.09s/it]\n"," video 1/1 (253/267) /content/gdrive/MyDrive/Classroom/SAE Workshop/YOLOP/inference/videos/1.mp4: <class 'tuple'> ((1080, 1920), ((0.35555555555555557, 0.3333333333333333), (0.0, 12.0)))\n"]}]}]}